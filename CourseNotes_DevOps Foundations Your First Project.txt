 Course Title: DevOps Foundations: Your First Project

Description: Curious about how to put DevOps principles into practice? This course can help. Join Carlos Nunez as he demonstrates how to apply DevOps principles to a sample app for a fictitious travel booking company. Throughout the course, Carlos shows how to leverage popular tools and methodologies to make updating, testing, and releasing the app a breeze. After explaining how to containerize the website with Docker and Docker Compose, he shows how to write unit and integration tests with RSpec, Capybara, and Selenium. He then steps through how to deploy your website into the cloud with Terraform and write a Jenkinsfile that will build, test, and deploy your app. Upon wrapping up this course, you'll know how to turn a slow and complex application development process into a fast and enjoyable one.


***********************************************
Chapter: Introduction
***********************************************


-----------------------------------------------
Video: Your first project, DevOpsified!
-----------------------------------------------
Note Time:         Note Text:                     

0:00:59            -> the difference between client and development environments. This course is an example to bridge the gap, by containerising a website, then running integration tests, deploying an instance of the site, then building a CI/CD pipeline in Jenkins 


-----------------------------------------------
Video: What you should know
-----------------------------------------------
Note Time:         Note Text:                     

0:00:33            -> the linux shell is a pre-requisite, IDEs/ using nano/ vim in the terminal for demos 


***********************************************
Chapter: 1. The Project/The Mission
***********************************************


-----------------------------------------------
Video: Your application, pre-DevOps
-----------------------------------------------
Note Time:         Note Text:                     

0:01:45            -> the example is a website Explore California -> orders are placed through the website and changes to the site are made through a lot of paperwork and process, production deploys are rare and take lots of time, changes need to pass through multiple environments before being deployed 

0:02:45            -> outdated operating systems because IT controls it 


-----------------------------------------------
Video: Your application, post-DevOps
-----------------------------------------------
Note Time:         Note Text:                     

0:00:37            -> implementing devops -> seeing survey and focus group information from marketing 

0:01:04            -> releasing changes to the website via a CI approach using github pull requests -> the entire philosophy is little and often (vs huge changes every number of months) 

0:01:18            -> automated changes through CI and CD pipelines 

0:01:49            -> designing a CD pipeline which runs failures and reports them to teams 

0:02:06            -> so - little and often, code tests and automatic via systems 

0:02:34            -> the results came from focusing on the process improvements, rather than fixating on the fact that the website wasn't working in comparison to it's competitor 


***********************************************
Chapter: 2. Testing Locally with Docker
***********************************************


-----------------------------------------------
Video: Installing Docker
-----------------------------------------------
Note Time:         Note Text:                     

0:00:23            -> installing docker -> running applications in isolated instances called containers 

0:00:53            -> containers are lighter than VMs and easier to modify 

0:01:21            -> docker containers run the same on different operating systems (e.g the same on the developers computer as the customer) 

0:01:37            -> they don't require package managers or compilers on the machine, which saves time 

0:02:04            -> docker containers are lighter than VMs, so we can have more of them / less chances of the site crashing 

0:02:17            -> docker compose links multiple docker containers together 

0:03:17            -> on the homebrew website -> brew.sh, there is a terminal command you paste in to install homebrew 

0:03:51            -> the installer is cloning repos and linking them together 

0:05:20            -> then install docker (brew cask install docker). The cask installs the entire thing rather than just the docker client interface -> packages from casks (third party repos) are installed 

0:06:08            -> then he opens the docker app (he's in a mac environment), give the containers higher level permissions 

0:06:22            -> there are containers loaded onto the docker whale 

0:06:49            -> installing a test container in the terminal via docker, then docker run hello world 

0:06:57            -> you can type docker in the terminal and it returns various functions 


-----------------------------------------------
Video: Writing your first Dockerfile
-----------------------------------------------
Note Time:         Note Text:                     

0:00:52            -> docker files lead to docker containers (which contain images). Docker reads and parses the files, fetches the parent image and runs commands within the docker file which are in the image 

0:01:21            -> docker containers contain all of the files the container needs to run an environment. Other methods e.g with chef require the writing of these files ('cookbooks') -> but docker containers contain the environment (cleaner) 

0:01:39            -> docker applications run regardless of the operating system of the computer they are being ran on, because they don't require these external configuration files 

0:01:41            -> in the docker file, all of the dependencies which the application needs will be in the docker file -> with chef, there are additional files ('cookbooks') which dictate the configuration of the container 

0:02:13            -> he makes a 'dockerfile' in the terminal via vim Dockerfile, and then edits it into the format of a dockerfile by writing it 

0:02:23            -> he's opening a docker file in vim in the terminal -> Dockerflie. He's writing a docker file -> which he's made. vim Dockerfile (in our case nano) 

0:02:30            -> you would type nano file_name to create this, and then edit in the terminal 

0:03:04            -> union file systems <- we have a starting image (which can be pre-existing from a repo), this is a parent image. Then, whenever we create different functions (operations with it), these are in different images. Layer these on top of each other in union and we have the image which the docker container uses 

0:03:25            -> it starts with FROM <- docker files are nested (it's telling the docker hub 'this is the starting image, model this new container off of it') -> a parent image, and images which stem from it -> each of these are layered on top of each other, and the whole is the union of these containers (the image) 

0:04:00            -> he finds a parent image to base the docker containers on -> by searching hub.docker.com (there is a docker hub, like git but for docker containers, and he searches for nginx, which is the parent image he is using) 

0:04:03            -> nginx from the public repo, then generating instances of containers based off of it 

0:04:07            -> to find a parent image, he's searching docker hub for nginx. There are a lot of different versions (flavours) of the nginx image 

0:04:58            -> back in the docker file (the blank file he made in the terminal and is writing into), he then states this parent image from the docker hub repo, and the distro of linux which it should use (originally found on the website) 

0:04:58            -> so, we say, we want a container, let's base it off of this parent container from a public repo on docker hub -> and in this version of linux 

0:04:58            -> there are different types (flavours, versions) of nginx which we can base the container on, each runs different linux distros. Alpine is the distro of linux he selects for nginx, which is engineered to be as lightweight as possible 

0:05:01            -> the hardware isn't virtualised -> we can run the same containers regardless of our operating system. There are different versions of software the images can run depending on the context (alpine is a distro of linux which is more lightweight) 

0:05:26            -> FROM nginx:alpile <- model this container on this parent image (nginx, which he's taken from the docker website): with the version using this distro of linux 

0:05:52            -> on a new line, MAINTAINER first_name surname <his_email@gmail.com> 

0:05:56            -> so, FROM nginx:apline <- the nginx docker image with the alpine linux distro. Then, MAINTAINER first_name surname <email@gmail.com> on a new line so other people can contact the maintainer 

0:05:59            -> then new line COPY website /website <- we're making a docker container which someone can pull. This creates a copy of the website for the project into a directory called website -> it's creating a version of the website on their computer which they have stored locally 

0:07:00            -> then new line, COPY nginx.conf /(a relative pathway) <- this is telling it where to store the configuration file 

0:07:13            -> wherever stuck, look on the documentation for the docker parent container 

0:08:38            -> he's done EXPOSE 80         

0:08:41            -> then new line EXPOSE <- this shows which ports (containers have ports and IP addresses) the container should expose to the computer it's ran on. Ports connect the containers to the computer the container is being ran on -> it's like, we have this VM (smaller VM), connect to the computer via this port in order to run the function contained on the VM 

0:09:00            -> to know what something does, you can look it up on the official documentation 

0:09:54            -> then in the terminal, he backs out of the text file when for making the docker file and docker build --help <- this returns a list of the options. docker build --tag website . <- and it starts building the website in the terminal -> it's copied the website, then the nginx configuration and now we need to expose the port to run the container 

0:10:46            -> docker run website -> the container doesn't have exposed ports, the ports on the container aren't being mapped to one from the computer - so the computer won't run the website which is held in the container 

0:11:43            -> docker run --publish 80:80 website <- publish takes the port on the outside of the container to the port on the mac (80:80). It's like a container is a small VM and we're connecting it to the computer's ports 


-----------------------------------------------
Video: Writing your first Docker Compose manifest
-----------------------------------------------
Note Time:         Note Text:                     

0:00:00            -> so we make a container (like a VM but smaller), that's like another machine but virtual. It has it's own virtual ports etc. We install software onto it via a docker file, which initialises the container with software from one on docker hub. Then, we download the website onto it, and link it's port with the one on the computer. Think of a separate machine, which has been loaded with the web application -> now we are linking its virtual ports to the computer's ones, so that it can be ran 

0:00:00            -> but the ports are virtual   

0:00:31            -> in this example, we now have a docker container to run the website. Docker build and run in the terminal to access and run it 

0:01:09            -> if we want to run a database on the website which is contained in the docker container -> we just have a container with the iso for a website, if we want to connect it to databases etc 

0:01:39            -> docker network command -> you can have the database on another docker container, and then connect them all through a network. Another alternative is docker compose 

0:02:09            -> docker compose -> running multiple containerised applications in a stack. docker-compose in the terminal <- this summarises commands which can be ran 

0:02:46            -> vim dockercompose.yml <- he's made a yml file to load the container onto which contains the website 

0:02:54            -> in the docker compose file (think of this as the file which is networking the containers, or will network them together), it's a yml file and he's editing it in the terminal 

0:02:54            -> so, we're using docker compose to run a stack of containers which are all linked by virtue of being made in docker compose - in comparison to making the containers, and then linking them through networking - it's something which automatically networks them 

0:04:40            -> top line, version: <- to find the verison he's looking it up on the internet -> look up the most current version (branch) of docker. Then new line services: <- services are what we want the docker container (or network of them) to do <- he's embedded website: // database: // application_server: // (// is new line). We're giving it all of the different things which are associated with this container for the website 

0:05:20            -> under website: he's nested build: in a new line -> rather than image - it's building a container, rather than taking one from an image on docker hub 

0:05:42            -> context: . <- the current directory -> build an image from a parent container in this directory we are already in 

0:06:41            -> then the ports, port: // - 80:80 <- link port 80 on the container to 80 on the computer the container is ran in 

0:08:31            -> then out of the file editor and in the terminal -> docker-compose run website <- it builds the website from the docker container. This is for one-off commands being ran in the container -> you would use docker-compose up to run all the services. Running one command might not require the ports being mapped -> to run the container / app, you need docker-compose up (to network the database for the website with the website e.g) 

0:09:04            -> how to find existing containers which are running -> docker-compose ps <- it returns a list of the containers running and their current statuses in the terminal 

0:09:26            -> docker-compose down <- this removes the container and the network that it used to run the containers in 


***********************************************
Chapter: 3. Testing Your App with RSpec, Capybara, and Selenium
***********************************************


-----------------------------------------------
Video: Setting up your test
-----------------------------------------------
Note Time:         Note Text:                     

0:00:11            -> I would define a test as the process of determining if something outputs the desired outputs. There are different ways this process can be implemented. Solving a Physics problem and looking at the answer to test if that output makes sense for the context, or the units do. Solving a business problem with code and testing if the code outputs the desired output. Testing a person to see if they are qualified. Whenever there is a problem, there is a solution - and a test is the process of determining if the solution is robust / trustworthy 

0:00:11            -> writing automated tests -> unit tests (chapter outline). A test is a file which is ran to verify if the code functions as intended, automating this is - writing these tests, and then having them run 

0:00:32            -> the process for this chapter is - writing unit tests and running them using docker tools 

0:00:48            -> RSpec <- Ruby based, DSL domain specific language -> it can write and read tests for websites 

0:02:10            -> Capybara <- a tool which uses a webdriver to interact with and test the website (e.g it can test if buttons are blue) 

0:02:27            -> Selenium <- a web driver which tests applications in a web browser 

0:02:58            -> in the terminal -> he makes a folder called spec, and another one called unit (a folder for the unit tests) 

0:03:53            -> in the folder for the tests, he creates a file (a test) using touch relative_path/file_name -> it means, in this file directory, create this file 

0:03:55            -> he opens the blank test file in vim -> require 'capybara' // require 'capybara/dsl' <- we are telling ruby what to use (capybara) 

0:05:01            -> then describe "....." do // code for tests // end 


-----------------------------------------------
Video: Setting up your Docker Compose service
-----------------------------------------------
Note Time:         Note Text:                     

0:02:10            -> he opens a file he made for docker compose in the terminal -> he is editing the file to include volumes, mounted the current directory to app. In docker, he's searched for rspec, and then proceeds to edit the file in the terminal 


-----------------------------------------------
Video: Creating the Dockerfile
-----------------------------------------------
Note Time:         Note Text:                     

0:00:15            -> he's made a new -> touch rspec.dockerfile and opened it in the terminal 

0:00:34            -> touch is for making a new file and allows you to specify more information about it 

0:01:40            -> FROM ruby:apline -> you can search docker hub for different options. Then MAINTAINER name <email> again -> then RUN gen install rspec capybara selenium-webdriver <- ruby gems are packages in the ruby gem repo which can be used to install libraries for ruby 

0:01:57            -> you can search for ruby gems by googling ruby gems / the repos 

0:02:48            -> then he installs helper libraries -> run apk add build base ruby-nokogiti <- we're adding another alpine repo -> this entire process is making a file in linux using the touch command, then populating that file with information (in this case telling ruby to install various libraries) 

0:03:45            -> ENTRYPOINT [ "rspec" ] <- you can use entry point and command (telling containers which stem from the parent container how to run). He then quits the file in the terminal and opens another one 

0:03:47            -> ENTRYPOINT [ "rspec" ] <- you can use entry point and command (twlling containers which stem from the parent container how to run). He then quits the file in the terminal and opens another one 

0:03:47            -> ENTRYPOINT [ "rspec" ] <- you can use entry point and command (twlling containers which stem from the parent container how to run). He then quits the file in the terminal and opens another one 

0:03:47            -> ENTRYPOINT [ "rspec" ] <- you can use entry point and command (twlling containers which stem from the parent container how to run). He then quits the file in the terminal and opens another one 

0:04:56            -> docker containers start with the same shebang line -> CMD [ 'rspec' ] # /bin/sh -c 'rspec' <- run rspec in a shell (its parent), this is in comparison to just using rspec 

0:06:09            -> you can give options to ENTRYPOINT (which means --option_name in the terminal) -> but with CMD this specifies the options as sh. CMD is preferred for running single commands, but for arguments you want to use ENTRYPOINT 

0:06:57            -> he opens another file (docker-compose.yml) in the terminal and edits it -> he adds in command: // - --pattern // - spec/unit*_spec.rb <- (where // is new line), rspec has a pattern to look for and it's in terms of a specific form 


-----------------------------------------------
Video: Running the test
-----------------------------------------------
Note Time:         Note Text:                     

0:00:10            -> so first, we write the unit tests -> and then we run them 

0:00:45            -> running a docker compose file -> docker-compose up -d website -> then docker-compose run --rm so that docker doesn't save the container when it's done. Then, docker-compose --rm unit-tests -> this runs the unit tests on the website 


-----------------------------------------------
Video: Identifying and testing an element
-----------------------------------------------
Note Time:         Note Text:                     

0:00:02            -> after building the image, it didn't run the tests (error message in the terminal), so he typed in the full path rather than the relative path in the test, ran it again and it worked 

0:00:16            -> there are some unit tests -> he's using capybara and rspec for testing the website. There are files with unit tests, and he's arguing against them and for the need of other tools 

0:00:24            -> so now we have a website and unit tests in a docker container -> linked with docker compose 

0:01:36            -> we want to write a test for a CSS logo -> in docker compose -> docker-compose config <- to validate if the file is correct or not (if it can be parsed) 

0:02:55            -> then docker compose up -d website to put in the background website. He tests this in chrome, localhost is the URL, and the website is there. There is a logo on the website, and he's inspecting it in the console. We want to test if the logo is actually there -> which we know it is. We're writing a test which tells us it's there, for the purpose of writing a test 

0:04:12            -> there is a capybara github page -> and it has a wiki / read me, including the DSL (domain specific language). He's copied code from the readme and pasted it into one of the testing files in the terminal. Matchers are used to check something is there -> to test if the logo is there, he's used expect(page.has_selector? 'a.logo') 


-----------------------------------------------
Video: Setting up Selenium
-----------------------------------------------
Note Time:         Note Text:                     

0:01:50            -> docker-compose run --rm  unit-tests <- it returns an error message -> so he's linking cappybara (for running the test) to selenium. He edits the test file to include selenium -> require 'capybara/dsl' et al 

0:03:27            -> Capybara.register_driver :selenium do |app|  <- he continues editing the test file, and refers to the capybara documentation on github -> linking it to selenium. There are multiple lines of code he adds to the test file which are inspired by the git page for linking them 

0:04:24            -> he has two files next to each other open in the terminal (split screen), and is explaining why one of the tests doesn't work -> one of the tests is ran in docker when the web application is launched 

0:04:31            -> in the docker container, instead of running a test and launching the web app at the same time, containers are preferred for one purpose 

0:05:09            -> for this, he creates a separate service using selenium to run chrome, and runs the unit test in the container -> he has a separate file which tells selenium where to look for the browser 

0:05:32            -> there is a url in the docker compose file to tell it where to look 

0:06:59            -> 'desired capabilities' in the docker compose file -> he's telling selenium to use chrome, and disabling the browser checks. Containers should each be used for one process 

0:07:37            -> he disables the way chrome checks for shared memory (shm) to stop it from crashing or thinking it doesn't have enough memory 


-----------------------------------------------
Video: Adding Selenium service to Docker Compose
-----------------------------------------------
Note Time:         Note Text:                     

0:00:00            -> he backs out of the file editor and runs the test in the terminal and attempts to run the tests, realises there is an error and fixes the syntax of the test file 

0:00:59            -> he creates a selenium service -> in the docker compose yml file in the terminal, he edits it. He searches the docker hub for the image we want to use for a container to contain selenium (we are making another container with selenium to run it in chrome, so the other container runs just the integration test for the website) 

0:01:26            -> there is a, on docker hub, a parent container called chrome debug -> which can be used to see the integration tests as they run in the container 

0:02:09            -> he's writing the file to produce the container for selenium -> this involves connecting the virtual port in the container to the one on the computer 


-----------------------------------------------
Video: Running your test with Selenium
-----------------------------------------------
Note Time:         Note Text:                     

0:00:07            -> then he makes the container (backs out of editing the file and docker -compose up -d website selenium) 

0:01:13            -> docker-compose run --rm unit-tests. He uses a client for screen sharing on the computer -> for connecting to local host 

0:02:20            -> there is an error he encountered and changes one of the integration test files <- going back and changing the services whenever errors are encountered 

0:02:32            -> then he runs the integration tests 


***********************************************
Chapter: 4. Infrastructure as Code with Terraform
***********************************************


-----------------------------------------------
Video: Creating the Terraform Dockerfile
-----------------------------------------------
Note Time:         Note Text:                     

0:00:37            -> explore california (the website) has a docker file which contains tests ran from rspec using capybara and selenium -> docker compose is connecting the website to the tests 

0:01:50            -> in the terminal -> Terraform is a golang based tool to deploy infrastructure into an environment (e.g Azure/ Google cloud etc) 

0:02:23            -> he makes a new file in the terminal -> a .Dockerfile file <- an image which when containers are run off of it can run terraform 

0:04:05            -> FROM apline <- a lightweight linux distro, then in a new line MAINTAINER (similar to before), then in a new line -> he goes into chrome, then types terraform, finds the download for it and copies the link address for the linux version 

0:05:37            -> then RUN wget pasted_link on a new line in the file he's editing in the terminal <- then he unzips the file with RUN unzip relative_path -d  / <- this extracts the version of terraform from the website into the path which is stated 

0:06:16            -> USER nobody <- the default user has no privilleges and is used execute certain things (to ensure sudo isn't used in a security breach) 

0:07:10            -> then in the terminal docker build -t terraform . <- and the code runs (the container is built) 

0:08:19            -> you can see in the terminal that terraform has been downloaded into the container -> then docker run --rm --interactive --tty --entrypoint sh terraform <- to send commands back and forth between the container 

0:08:53            -> then /terraform to check if the application has been installed 


-----------------------------------------------
Video: Building and testing a Terraform Docker image
-----------------------------------------------
Note Time:         Note Text:                     

0:00:50            -> testing the docker file -> using docker build, docker --help <- there is an option --tag which can be used to find the docker name 

0:02:25            -> so docker build --tag terraform --file terraform.Dockerfile, then a context (docker copies whatever the docker image asks for into this context, as a temporary location - like a working directory) -> and the context is . (the current working directory) 

0:03:03            -> he runs the code and the container forms -> it takes the file from the web, unzips it and moves it into the root directory 

0:04:17            -> docker run --help <- this is what he uses to search for various options. So, docker run --rm terraform -> this returns a list of commands which can be run, with global options 


-----------------------------------------------
Video: Creating a Terraform Docker Compose service
-----------------------------------------------
Note Time:         Note Text:                     

0:01:08            -> then he installs docker compose -> in a yml file, he edits it -> to contain build and context -> we're building the container as a context. Then docker-compose run --rm terraform <- this is to run the container 


-----------------------------------------------
Video: AWS deployment explained
-----------------------------------------------
Note Time:         Note Text:                     

0:00:00            -> then docker-compose build terraform -> he's installed terraform within docker and then ran it within docker compose 

0:00:42            -> there is a docker file to run the website locally, unit tests ran with rspec and using docker compose to network these parts. There is also a service to run terraform 

0:01:12            -> this is using the terraform to run the website onto AWS (amazon web services). This is a cloud provider which enables us to create processes etc to use in Amazon's data centre 

0:01:45            -> EC2 <- elastic cloud compute, this can be used to create VMs in AWS. There is also Amazon S3 (the service used in this video) and are other examples he lists 

0:02:24            -> services can be interacted with through API callings. There is the AWS console, it can also be accessed through command line clients, development kits in different languages 

0:02:52            -> this is using the S3 storage service -> copying the website into it, into a bucket (a folder in a file system). Objects in buckets are called keys (in this example our website) 

0:03:15            -> S3 can host websites without additional infrastructure -> a host bucket can be a static website 

0:03:30            -> you can copy a website into S3, and then configure so it's available on the internet 

0:03:48            -> creating an S3 bucket, then configuring it as a website, copying it into the website and reconfiguring it so it's available on the internet, and testing it 


-----------------------------------------------
Video: Writing Terraform code
-----------------------------------------------
Note Time:         Note Text:                     

0:00:42            -> terraform (configuration) files <- .tf extension, the entry point in these files starts with main.tf. He's made one in the terminal (vim main.tf). Terraform can read tf files 

0:01:04            -> then editing the tf file in the terminal, he's going through the settings in a terraform file 

0:01:38            -> we have a file, and he's breaking it down / into each of the parts of it. There is terraform documentation at terraform.io > docs > terraform language documentation 

0:03:17            -> Terraforms use providers, which can bridge between different cloud platforms -> the providers are split into different cloud providers, and documentation for the different providers can be searched on the terraform website 

0:03:49            -> in the terraform file, there is a resource -> this is to create infrastructure (resources allow the creation of infrastructure) 

0:04:01            -> expose one or many more resources 

0:04:25            -> so there is a section in the terraform file which lists the resources, one is a bucket (this creates buckets using the AWS S3 service). S3 is like a large file system, and buckets are like boulders in the file system 

0:05:02            -> you can make buckets accessible to the internet as websites. Resources take arguments -> like the arguments in functions. All resources have different sets of arguments / parameters 

0:05:44            -> it is listing out which website is served out of the bucket -> and there can be many different arguments 

0:06:36            -> another part of the terraform file is the outputs section -> this can be used to return information 

0:07:15            -> another section of the tf file is the variable section <- these variables (in this case domain name) can be referred to outside of terraform 

0:08:08            -> variables can be provided default values -> you can change the value of variables at run-time 

0:09:25            -> another part is the data block -> these allow information about existing data structures to be looked up (among others). Data providers -> e.g IAM policy documents (identity access management) <- this defines how the bucket is accessed (by who etc) for security 

0:10:26            -> IAMs can be used to create templates about existing infrastructure which can be rendered elsewhere (these are written in this example in json) -> allow everyone to access the page when it's a website (a bucket policy argument) 

0:10:42            -> binding policy arguments to buckets 

0:10:46            -> exposing json attributes which are later used 

0:11:13            -> you can use terraform files to create templates (we want to read the website which is contained in a bucket, and then in the policy section of the website, expose certain attributes -> we are binding policy documents to the attributes) 

0:11:18            -> every resource can expose attributes -> these allow us to acquire information about the resource 

0:11:45            -> website endpoints are urls which the websites contained in the buckets will be referenced by 

0:12:35            -> in the documentation, there is more information about terraform / buckets 


-----------------------------------------------
Video: Reviewing the Terraform plan
-----------------------------------------------
Note Time:         Note Text:                     

0:01:22            -> in the terminal we are deploying infrastructure. This requires an initialisation -> cocker-compose run --rm terraform init <- this file contains information about the entrypoint -> we are parsing as terraform 

0:02:24            -> initialising terraform initialises something called the terraform magnet (the terraform state), it's a large graph which contains information about the relation between different resources etc to create a plan 

0:03:21            -> the terraform state is by default stored locally -> which can cause conflicts if the computer is compromised 

0:04:23            -> terraform can use the backend to handle retrieving to the backend of the state 

0:05:46            -> we can also download plugins for terraform 

0:07:21            -> root ch certificates -> he's editing a file in the terminal. Adding certificates through root authorities -> they need to be copied / hosted in specific locations 

0:09:11            -> APK is a package manager for apline linux -> he's added into the file, a line which says apk add ca-certificates -> then back in the terminal, docker-compose build terraform <- this updates the layers of the container according to the new changes 


-----------------------------------------------
Video: Applying the Terraform plan
-----------------------------------------------
Note Time:         Note Text:                     

0:00:19            -> make a terraform plan <- a pre-view of the deployment terraform will do before it runs it 

0:01:46            -> AWS asks for credentials (keys and paths), in this example there is a data centre -> and they are in different regions. e.g us-east-1 <- and then -1a,-1b, -1f etc depending on which data centre we want to use 

0:03:28            -> vim docker-compose.yml <- providing environment variables -> he's editing a file in the terminal. Under one line of code (the 'environment' line), he's adding in a AWS_ACCESS_KEY_ID: ...., and on a new line there is AWS_SECRET_ACCESS_KEY, another is AWS_REGION and AWS_DEFAULT_REGION. He's essentially added a few new environment variables and then is running the plan in the terminal (docker-compose run terraform plan) 

0:03:54            -> terraform looks at a state file to see what was made before, and compare it against what files we have locally - then seeing what provisioning changes need to be made 

0:04:36            -> terraform states -> these can track infrastructure with the state (information about how the infrastructure changes is logged in a file in terraform). This is why you run a terraform plan 

0:05:19            -> in one of the terminal files, it's created a bucket, retrieved data and put it into the bucket. It can also remove things (-) or create them (+) 

0:05:45            -> he's updated the changes with docker-compose run --rm terraform apply. This costs (pay per use) 

0:06:19            -> on the AWS website, you can go through a sign up process which involves an access key 

0:06:20            -> on the AWS website, you can go through a sign up process which involves an access key 

0:06:21            -> on the AWS website, you can go through a sign up process which involves an access key 

0:07:04            -> so he's created the bucket in the terminal for AWS -> copying the website into it should launch it (the bucket on AWS can be accessed online) 


-----------------------------------------------
Video: Deploying the website into AWS S3
-----------------------------------------------
Note Time:         Note Text:                     

0:00:01            -> using AWS cli in order to -> so, we have a bucket on AWS (this can be accessed online) and now we're copying the website into it to launch it online 

0:00:58            -> in a docker compose yaml file file in the terminal, he edits it and adds a new service -> he takes this from an image on docker hub (he searches it for AWS cli and selects the most popular option, then links to it in the file) 

0:01:57            -> then he runs the file -> docker-compose run --rm aws <- it's pulling the image from docker hub. In this case it has no entrypoint (the default is sh) 

0:02:15            -> we know there was no entrypoint because it didn't run error messages. docker-compose run --rm entrypoint aws aws. Then he makes a test for it in the terminal -> this is using the ec2 service 

0:02:53            -> he runs code to return the name of the bucket (docker-compose run --rm terraform output). It returns the url of the bucket and the name of the bucket -> the name of the bucket is associated with the record which is internal to AWS 

0:03:47            -> now we want to use the AWS s3 command to copy the website into the bucket. First, s3 help (which returns and error), he fixes the error 

0:05:13            -> then docker-compose run --rm --entrypoint aws aws s3 cp --recursive website/ url_name <- then he runs the code which allows the client to copy the website into the bucket 

0:05:48            -> then he checks it's worked by going onto the website for the bucket, which the website was copied into - and it's running on the internet 


-----------------------------------------------
Video: Destroying the website from AWS S3
-----------------------------------------------
Note Time:         Note Text:                     

0:00:23            -> using terraform to remove the website -> the terraform destroy command 

0:01:46            -> he's using the AWS cli to remove the bucket (which contains the website we want to take down) -> terraform destroy (it's a longer line of code), then confirming he wants to delete the bucket 

0:02:41            -> he can't delete the bucket because it contains the website -> so he's first removing the website then the bucket. He goes in and delete the individual files for the website in the bucket (docker-compose run --rm --entrypoint aws aws s3 rm url_of_bucket 

0:02:57            -> this deletes everything from the bucket in the terminal 

0:03:20            -> s3 is a global file system -> s3 is segmented by region, but the keys used are unique to the globe (there is one key) 

0:03:57            -> when you delete the bucket, he's changing the name to prevent naming collisions (there can only be one bucket with each name) 

0:04:10            -> then control r destroy -> he's deleting the bucket (so first it's contents and then it) 

0:04:49            -> ls -la <- to see the different permissions and files in the service -> there is terraform,tfstate and terraform.tfstate.backup -> don't save the terraform state in a local directory 

0:05:49            -> if you are working with a shared code base, you need to be careful with where the files for the terraform state are stored (security). There is also a remote state 


-----------------------------------------------
Video: Writing your integration test
-----------------------------------------------
Note Time:         Note Text:                     

0:00:39            -> explore california (the website in the example) -> we have a a series of local tests, and we can deploy the system in the cloud 

0:02:41            -> integration tests -> he's creating a test (this is a file). In the terminal, he's made a folder for the integration test, and is editing a file in vim. He's changed one of the urls in the file -> to an environment variable. Back in the terminal, he's running terraform 

0:03:09            -> we don't know what the website url will be, so adding it into terraform doesn't make sense (he's defining it afterwards) 

0:03:42            -> docker-compose run --rm terraform plan <- running terraform to deploy the infrastructure. Terraform uses a state file to keep track of the infrastructure 

0:04:15            -> he's opened a tf file in vim, then backed out of editing it. It reads the tf file, checks what exists on the cloud, then creates difference of what is in the file system in comparison to on the cloud 

0:04:47            -> IAM - identity access manager, this specifies how and what the public can access and who can see how the bucket is permissioned etc 

0:05:30            -> then docker-compose run --rm terraform apply <- then the terminal asks if we are applying the changes or not. So, we're creating a bucket -> and its url is outputted at the end of this process 

0:05:53            -> then he's adding the integration tests in with the terminal. Modify the docker compose file -> he's adding the integration tests in by editing a yml file in the terminal 


-----------------------------------------------
Video: Running your integration test
-----------------------------------------------
Note Time:         Note Text:                     

0:00:56            -> then back in the terminal, he's running the integration tests -> through first starting off with selenium, which is an application open in another window. Then docker-compose run --rm integration-tests 

0:01:59            -> he tried to run the integration test, but entered the website incorrectly (https://) <- in other words, if code doesn't work when ran look at the error messages 

0:02:26            -> the website needs to be copied into the bucket 

0:03:20            -> to delete the site, he's used the destroy command 

0:03:21            -> the tests for the website can be ran in the terminal -> then he removes the website after running it 

0:04:06            -> docker compose up is for services which are accessed for longer periods of time. In order to delete the bucket, he first deletes the content 


***********************************************
Chapter: 5. CI/CD as Code with Jenkins
***********************************************


-----------------------------------------------
Video: Installing Jenkins on Docker
-----------------------------------------------
Note Time:         Note Text:                     

0:02:15            -> continuous integration -> this is an automated alternative. Github actions is a newer way of doing this. Jenkins is an older method of doing this -> this has a large library of plugins. We can write files for Jenkins in JS / ruby 

0:05:01            -> he creates a file in the terminal for docker -> FROM // MAINTAINER -> he searches the docker hub website for a pre-existing image. He finds a Jenkins server on the website, and selects the alpine version (which is a more lightweight distro of linux) 

0:07:12            -> a tag is a version of the image -> in the file in the terminal, he adds a maintainer line, copies plugins (these are for Jenkins) <- there is a file called the plugins file which it installs and the ones we want to install are stated 

0:08:49            -> there is an online repo of plugins (plugin index), he writes a file with all of the different plugins we want (workflow-aggregator, seed, git) 

0:13:27            -> then back in the other file, he links the file with the list of plugins we want, then EXPOSE 8080. He opens another file in the terminal for docker compose. He adds context, dockerfile (jenkins), creates volumes (states the directory for them), states the current directory. He then exposes the port (ports: // 8080:8080 <- linking the port from the container to the one on the computer) 

0:16:22            -> he then runs the plugin script -> which installs plugins and then starts Jenkins. He goes to localhost: 8080 in chrome and it shows Jenkins running. There is a key in the Jenkins build in the terminal -> he enters this into the page on Chrome 


-----------------------------------------------
Video: Writing a Jenkinsfile for the app
-----------------------------------------------
Note Time:         Note Text:                     

0:00:14            -> creating a Jenkins file to deploy a website -> this is Jenkins in docker 

0:01:39            -> there is a Jenkins file he's editing -> he's just made one in the terminal. There is a declarative and scripted pipeline -> the difference is syntax and flexibility. Scripted pipelines are closer to Ruby syntax but are less readable than declarative pipelines. Declarative pipelines also have more documentation 

0:02:18            -> in the Jenkinsfile.yaml in the terminal -> pipeline {} <- and in the block it's agent any // stages {}. Everything in the pipeline happens within the pipeline block of code 

0:03:05            -> when you log into Jenkins, you're logging into a Jenkins master, and its nodes which run implementation are the 'slaves.' In the pipeline block, it's agent any <- Jenkins can use any agent on any slave to build the job 

0:04:38            -> stages <- these define the stages which need to run and are default step by step. He has added three lots of stage{} within a stage{} command, each of these contain 'Text' with the name of the different stages of the deployment 

0:04:59            -> there can be a separate pipeline which runs integration tests separately 

0:05:41            -> nested under the stages, he has listed under steps {} <- sh "a_relative_path_to_a_.sh_file". This invokes a shell and runs this script when running the Jenkins file. He copies the process for the three 'stages' -> build the website, run the unit tests, then deploy the website 


-----------------------------------------------
Video: Using Jenkinsfile to deploy your app
-----------------------------------------------
Note Time:         Note Text:                     

0:00:00            -> then in the terminal, docker-compose up jenkins -> this shows logs as the script is ran. In Chrome while this runs, localhost:8080 (this is where Jenkins is installing the website, we can see the install running in the terminal) 

0:01:15            -> there is an administrator password the terminal produces which he pastes into the page on Chrome 

0:02:10            -> C jobs show Jenkins files in a repo 

0:02:10            -> in Chrome, he starts using Jenkins -> which shows the Jenkins console. H creates a C job to look at repos and find Jenkins files in their branches. In this example, we have one branch 

0:03:30            -> so he's in a repo, has initialised it as a git repo, created a branch for the files then moved the files into it 

0:03:30            -> git commit -m "Initialise repository." <- adding a tag to the commit 

0:03:30            -> then he's adding the the files into the branch -> git add -A (he's committing all in one go) 

0:03:30            -> he's initialising the repo as a git repo -> git init. Then he's initialising a branch to put the files -> git checkout -b master, this is the master branch 

0:03:30            -> in the Docker file, it's copying plugins and scripts for them, and then exposing the port for the container 

0:03:30            -> in the Jenkins file, it contains a pipeline and three stages (build, test and deploy) 

0:03:30            -> he's in the terminal, ls -l for the repo contents. There is a yml docker and a yml Jenkins file 

0:03:30            -> taking an example Jenkins file and running it <- this is for running continuous integration pipelines 

0:03:59            -> in the Jenkins webpage, enters the item name (the name of the website) and uses a multi branch  pipeline 

0:05:50            -> he enters a display name (there are two types of name, a display name and job names in the URL),  a description. You can add branch sources -> mounting a Jenkins repo onto it which is stored on the computer (if this was from github a different address would be used) 

0:07:11            -> there is a secret path which can be used 

0:08:15            -> there is a part of the webpage which shows the status of the tasks which are running 

0:08:15            -> he runs Jenkins, which is searching for the master branch 

0:09:51            -> it shows the errors after running the code, something being wrong with the Jenkins file. He goes back to the file in the terminal and edits it to fix the error message -> a lot of back and fourths. He changes the file, then git status to see the change, and git commit to commit the changes 

0:11:03            -> he enters a summary of the changes which have been made in the terminal -> he's treating it like a github file (this is what he did at the start of the process, made the directory into a github repo). But this also involves treating the file like a github file whenever changes are made -> writing a description of the changes he's pushing 

0:12:05            -> he signs back into Jenkins on the webpage and runs the test again -> which passes this time 

0:13:06            -> the changes show in the console log in Jenkins. So the process was, take the jenkins file, modify it, load it into Jenkins, debug it and looking at console logs 


-----------------------------------------------
Video: Challenge
-----------------------------------------------
Note Time:         Note Text:                     

0:00:00            -> deploying infrastructure into AWS with terraform 

0:00:00            -> terraform -> create, change and keep track of code infrastructure 

0:01:28            -> then creating a docker compose service called terraform to create the service, and which uses apline linux 

0:01:28            -> stages to create VMs in AWS EC2 file (elastic compute) -> write a main.tf file which creates a VM in EC2, using the AWS ec2 instance 

0:01:45            -> then using bind mounts to mount the current working directory into a working directory called / work 

0:01:55            -> create an AWS account -> these have credits on a free a account 


-----------------------------------------------
Video: Solution
-----------------------------------------------
Note Time:         Note Text:                     

0:04:58            -> so we've inspected terraform.Dockerfile 

0:04:58            -> ENTRYPOINT terraform, and USER nobody <- for security concerns (this is not USER root) 

0:04:58            -> then RUN unzip terraform.zip -d / <- unzip the thing which was downloaded (-d to extract into the root of the file system) 

0:04:58            -> in another line, RUN w get -0 terraform.zip and the url of a container from docker hub we're using as the parent container (this can be searched from online) 

0:04:58            -> he's added a maintainer with the contact details of the file creator 

0:04:58            -> he opens terraform.Dockerfile in the terminal -> it's a short file. FROM apline <- we are building containers on top of other parent containers -> it's inheriting the version of alpine linux 

0:04:58            -> looking at the files using ls -> there is a terraform.Dockerfile (this runs terraform and creates the VM), docker-compose.yml (this defines the terraform service to create the container which runs terraform from the docker file, and is the service from which the operations are ran), main.tf (this is the file which creates the VM) 

0:04:58            -> solving the terraform VM challenge 

0:05:08            -> docker build -f terraform.Fockerfile -t terraform . <- this makes docker build the image in terraform 

0:05:52            -> docker run --rm terraform version <- this returns the version of terraform which is being ran 

0:06:06            -> the nest file in the demonstration is docker-compose.yml <- the file which defines the terraform service 

0:07:02            -> he opens the file in the terminal -> the top line is the version for the compose manifest schema being used (the most current version), then the services being used. Nested under services is terraform, build, dockerfile, context and volumes. We are stating for it to build a docker file from an image 

0:07:42            -> bind mounts / volume mounts -> volumes hard-disk images for a virtual machines to allow something to be accessible for the machine (this is the volume keyword in the file) 

0:08:20            -> he looks up documentation for volume mounts on docker docs -> this is more information on what volumes are and how they work. There is example in the documentation similar to the file int he example 

0:09:22            -> 'outside colon inside' .:/work <- from this current working directory . mounted to the /work directory. We're mapping directories to each other 

0:10:01            -> then there is a section in the document with options nested under 'environment' <- these are access keys and region variables listed in a certain format 

0:10:51            -> he's running the docker-compose run --rm terraform version <- to return the version of terraform and if it's running 

0:13:02            -> he opens a third file in the terminal (terraform.tf) and googles the documentation for aws_instance in terraform. There are examples in the documentation which he is comparing to the file in the terminal -> naming a VM, giving it a size, telling EC2 what image to use for the VM 

0:14:01            -> AMI <- amazon machine image ID. He is going through the documentation and describing what one of the examples does -> creating a VM which runs ubuntu 

0:14:28            -> he's comparing the documentation example to the code we have open in the terminal 

0:14:43            -> then we're running docker-compose run --rm terraform init -> this fetches provider plugins in the terminal 

0:16:13            -> docker-compose run --rm terraform plan -> this is running a plan to deploy the code which we want to. He is checking its output before running the job -> it returns a longer string am1, checking that the instance type it returns is the size we want, checking the tags, name 

0:18:04            -> then he runs terraform apply, and is being asked if we want to apply the changes from terraform plan. He opens the AWS console in a Chrome page, runs the code in the terminal so terraform creates the VM and then refreshes the Chrome page in AWS so we can see the VM being created 

0:19:04            -> he uses terraform destroy to remove the service. We also have a public and private IP address 

0:19:41            -> then in the AWS Chrome page we can see the service gone (so we delete it in the terminal using terraform destroy, and can see the changes being made on the AWS Chrome webpage) 


***********************************************
Chapter: Conclusion
***********************************************


-----------------------------------------------
Video: Beyond your first DevOps project
-----------------------------------------------
Note Time:         Note Text:                     

0:00:22            -> the explore california website being ran in docker, tested in rspec and selenium, automated CI/CD in Jenkins and deploying it locally onto AWS in terraform 

0:01:47            -> suggested further developments from the project -> hosting the website on a container orchestrator (taking the container concept to production), creating a manifest, submitting jobs into an orchestrator, docker swarm (the orchestrated version of docker compose) 

0:02:03            -> hashicorp nomad -> this can handle higher workloads 

0:02:14            -> kubernetes is another example 

0:02:50            -> a further option to develop the project is more docker security (for containers which run from the root), doing things as root from the host on the container 

0:03:26            -> using the user command in the files to restrict the access, although this can be overran with the --user option. Controller (c groups) and namespaces to control what containers have access to and how much access they get to those resources 

0:04:18            -> another extension is automating CI/CD builds without having to use the command line (every time we make a change to the Jenkins file, running automatic tests / web hooks through a plugin in Jenkins) 

0:04:39            -> making a pull request to merge / continuous integration 

0:05:03            -> deployment strategies when something fails -> rollback plans 

0:05:55            -> canary (shipping a portion of the website into production, waiting until everything is clear), blue green (two instances, the older and the newer version of the deployment and switching between them if one fails) and rolling deployments (deploying the entire website to a small population at first) 

0:07:11            -> good books -> the phoenix project, pro git, terraform: up and running 

0:07:42            -> continuous delivery is another book -> the need for CD 

