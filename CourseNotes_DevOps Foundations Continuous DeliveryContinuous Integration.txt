Course Title: DevOps Foundations: Continuous Delivery/Continuous Integration

Description: Continuous delivery is one of the major DevOps practice areas. By continuously building, testing, and delivering your code, you can reap huge stability, speed, and flexibility benefits. In this course, learn about continuous integration and continuous delivery (CI/CD), and see how these concepts work in practice by constructing your own build pipeline. Throughout the course, instructors James Wickett and Ernest Mueller discuss elements of the pipeline as they show how to take an app written in the Golang programming language from development to production. They walk through version control, building artifacts, unit testing, and deployment, demonstrating common practices and tools along the way. They conclude with some practical advice on CI/CD best practices and common pitfalls, workarounds, and compromises that you may encounter on your journey to continuous delivery.


***********************************************
Chapter: Introduction
***********************************************


-----------------------------------------------
Video: DevOps foundations: CD/CI
-----------------------------------------------
Note Time:         Note Text:                     

0:00:25            -> benefits of devops -> more structured software delivery pipelines, more continuous software releases and updates, software which is more resilient against failures 

0:01:24            -> continuous integration, delivery and deployment, elements of the pipeline -> source, building, artefacts and developments, and concepts in devops 


-----------------------------------------------
Video: What you should know
-----------------------------------------------
Note Time:         Note Text:                     

0:01:21            -> concepts of continuous integration and delivery -> devops fundamentals -> infrastructure automation, continuous delivery, reliability engineering. Course requirements -> bash, applications knowledge. This is a concept based theoretical course 


***********************************************
Chapter: 1. Continuous Integration and Continuous Delivery
***********************************************


-----------------------------------------------
Video: DevOps core concept: CI/CD
-----------------------------------------------
Note Time:         Note Text:                     

0:01:18            -> continuous integration (automatically building and unit testing an application, constantly -> we are writing new code and integrating it in, and to integrate it in, we test it), delivery (publishing the code in a production like environment) and deployment (this can happen on virtual machines) 

0:02:32            -> continuous deployment -> every change going through automated testing and being added to the software in the production pipeline. There are build and release processes which involve hundreds of changes -> merging conflicts. Some teams work their way up to continuous delivery -> but continuous delivery is a large part of devops engineering 


-----------------------------------------------
Video: Benefits of continuous delivery
-----------------------------------------------
Note Time:         Note Text:                     

0:01:57            -> investing in continuous delivery and integration -> empowering teams (the pipeline makes the code production system transparent -> all teams know how the pipeline works, it's to do with the culture of the team). Another benefit is lower cycle times -> cycle times (making a change to the code and integrating it) happen within hours rather than entire weeks 

0:03:55            -> another benefit is security -> continually integrating in security with each integration cycle. There are steps which are well tested and known to deploy the code -> code commits used to happen after large meetings. Another benefit is time to be productive -> there is more time to add new features (less time re-working code due to integration issues) 


-----------------------------------------------
Video: Build pipelines in practice
-----------------------------------------------
Note Time:         Note Text:                     

0:04:38            -> build pipeline <- infrastructure to create and deploy the code. Start with a source code repository, move to version control, then build systems (these watch the repositories and trigger changes when required, these also provide feedback and visibility in the process), then build tools (these can work as command line integration tools), then unit tests (these can be separate tools or integrated in, and are used to test the functionality of the code), then artefacts (package what was made into an artefact), then artefact repos (some people just use storage devices, or specific tools for this), then deployment servers and tools (these build up working instances), then integration tests, end to end tests, then production environments (this can involve the use of acceptance tests) 


***********************************************
Chapter: 2. Build Your Own Pipeline
***********************************************


-----------------------------------------------
Video: Introducing the delivery pipeline
-----------------------------------------------
Note Time:         Note Text:                     

0:02:39            -> sample continuous delivery pipelines -> example with open source software -> version control (git), then build systems (Jenkins), then artefact handling, testing (unit, integration), deploying it using Chef. You can make CI flow diagrams to show the pipeline for producing code 


-----------------------------------------------
Video: Version control practices
-----------------------------------------------
Note Time:         Note Text:                     

0:02:13            -> version control -> source / revision control -> it contains the source code for the project, including the current and historical versions. Best practices -> use version control (like managing the different drafts of the projects), commit small batches of code frequently to aid in collaboration, use clear and simple commitment messages to describe the changes which have been made 

0:05:18            -> more best practice - don't commit code which is broken or fails tests - only code which passes tests should be committed. Branches -> they recommend a master branch approach -> frequent branch life and integration for productive teams. Branch by abstraction -> use flags in the code, making small changes to the code frequently. Hooks enforce quality (this can be done in git), linters and formatters, running code locally. Maintain privacy -> source control can be searched 


-----------------------------------------------
Video: Version control in action with Git
-----------------------------------------------
Note Time:         Note Text:                     

0:01:52            -> version control with git -> he's in a mac terminal. Make a github account with ssh keys, then he git clone ... <- clones a repo in the terminal. Then he cd's into the repo, git log can be used to see all of the past versions of it. git show ... <- for a description message of the commit 

0:04:49            -> he changes the readme file using vim. git status -> git diff can be used to see what changed. Telling git how to take certain actions -> he's still in the terminal -> cat .(a relative file path). He's written code which deliberately violates rules which he's set in a function -> and then trying to commit a change using git add ... then git status -> then git commit fails the test 

0:07:09            -> he's made the change to the code in the terminal and the change can be seen on the git page. There is a page for issues in git when committing changes. We also have settings for webhooks -> comit, branch and pull requests are checked 


-----------------------------------------------
Video: Continuous integration systems
-----------------------------------------------
Note Time:         Note Text:                     

0:01:26            -> continuous integration build systems -> #1 open source <- Jenkins, download the software and post it, #2 commercial (purchased) and #3 CI as a service (pay per use or per month) <- sas is recommended and in larger organisations the decision will have been made 

0:03:17            -> best practices -> start with a clean environment instead of reusing them from previous tries, build to pass the coffee test (the time it takes the test to run should be 5 minutes, continuous integration), a continuous integration culture -> run tests locally before committing the changes, don't commit new code to broken builds (fix the build first), don't leave the build broken, don't remove tests that fail (change the reason the test is failing) 

0:04:00            -> use notifications to update your build progress (slack for code start, build completion) 


-----------------------------------------------
Video: Continuous integration in action
-----------------------------------------------
Note Time:         Note Text:                     

0:03:08            -> Jenkins (open source continuous integration tool), written in JS. There is a Jenkins docker container. In the terminal, he cd's into the docker, he's using the terminal to look at the different files in the docker -> he's installing different tools on top of the container, then he's running a docker compose file (this describes a set of containers), then docker-compose up --build -d, and the install process begins 

0:05:28            -> so Jenkins is installed in the terminal, then docker-compose ps, he's gone onto the Jenkins website and logged in -> there are different projects, including a word cloud generator in this example, there are build triggers (it pulls the repo every x unit of time to update), then it uploads the code to nexus 

0:08:29            -> Jenkins works using plugins, which includes the nexus plugin. There is a global tool configuration, and under go different installations can be completed. He's added a golang plugin / configuration. Jenkins.io has a plugins index -> the more plugins the more updates have to be managed (only install what you need). Next to each project, you can run the project in Jenkins 

0:10:06            -> he's ran a build in Jenkins and there is information from the console output -> it has read the source code, built it, tested it, zipped it and exported it. You can add slack updates. There is a Jenkins pipeline feature -> we're using it to build and test code as quickly as possible (and automating this) 


-----------------------------------------------
Video: Building artifacts
-----------------------------------------------
Note Time:         Note Text:                     

0:02:38            -> packaging and artefact management -> building and distributing code as artefacts -> ensure what you've tested is what goes into production. Test as much as possible -> building and testing -> every time a new version of the code is made, the code needs to be re-ran. Composability -> most apps are composed of more than one piece. Packaging code into different versions 

0:03:45            -> you can have multiple layers of artefacting -> different ways of processing the artefacts. Security -> not exposing everything in the source code repos to production servers. Being careful about what goes into the package, and what code is pushed to production -> only letting production deployments come from the artefact repo and only allowing the CI system to write to the artefact repo 

0:05:13            -> you can make external artefacts with the repo. Knowing what went into the build. Another benefit of having an artefact repo is that the code can easily be shared -> being careful about organisation of the code (and planning it) before producing artefacts 


-----------------------------------------------
Video: Artifact repositories in action
-----------------------------------------------
Note Time:         Note Text:                     

0:02:01            -> what are artefact repos and how are they built -> nexus repo. In the terminal -> he's printed out information about a container (it's a nexus container) and linking to a certain port. Then docker-compose up  --build -d <- he's made (composed) a container in the terminal. docker-compose logs <- there are messages coming through about the status of the container production 

0:03:28            -> he's opened the nexus website in Chrome and logged into it. Then we're browsing through the different assets -> they're different files in the nexus browser. He's gone into one file, and open is a word cloud generator and other files to test it. It looks like git -> there is a section with different repositories 

0:06:14            -> integrating nexus (a bit like git) with jenkins -> on the jenkins website, there is a plugin for nexus. Then he's configuring the build on the jenkins website -> nexus artefact holder. The version number -> a major number and build number (unless the code is being publicly released). E.g 1.$build_number <- 1.1, 1.2, etc in the version number naming, and each one should be separately tested / traceable 

0:07:57            -> then in nexus, in the file for the word could generator, there is a new artefact (file) which was created using the jenkins website. Then he's pulling the file in the terminal -> ssh root@localhost, then the password -> then curl -v .... <- then it's pulled the word cloud generator zip file. To test it, md5sum word-cloud-generator-1.1.1.gz -> this returns a number which can be entered into the jenkins webpage, then it returns the file 

0:09:30            -> gz is a zipped file, he's pulled a zipped file. gunzip ...., ls -; the chmod -> so he's unzipped it and changed the permissions -> then opened it in chrome. We can setup limited permissions, or go back to older artefacts (file versions) -> this is an artefact repository (which isn't git). This can be used to look for security flaws, they can also be used as file servers to hold the artefacts 


-----------------------------------------------
Video: Testing and continuous delivery
-----------------------------------------------
Note Time:         Note Text:                     

0:02:15            -> continuous integration and delivery -> you need to perform continuous, automated tests. We want tests to be fast, reliable, and to tell us where the failures in the code are. Source control, artefacts <- different terminology, each test has different terminology -> unit (testing a unit of code without external dependencies, modules or libraries), integration (when pieces of the application are integrated and brought together), UI (end-to-end testing, testing the code the way the user could use it), security (to prevent data leaks) 

0:04:09            -> soak, spike, step, system, acceptance, validation tests. Shift left (moving testing to the beginning / front of the pipeline, move it to the front), test fixture (objects used to run tests in a well known environment -> a fixture is like a tap, it's a facility to do the test), system under test (SUT, a system being tested), cycle time (how long from working on the code to the production deployment), lead time (time in between requesting an item to delivering it into production -> the time to run all the cycles, generating a lead), mocking (mocks of code which can be simulated, e.g mock classes) 

0:04:20            -> stubs, dummies <- these are examples of mocking 


-----------------------------------------------
Video: Testing philosophy
-----------------------------------------------
Note Time:         Note Text:                     

0:00:59            -> test philosophy -> how you use the tools. TDD <- test driven development -> writing a failing test first, then the code which causes the test to pass, then refactoring to make it cleaner. Behaviour-driven development (BDD) -> writing tests in an end-user and behaviour-centric language (the tests for the code are written with the behaviour of the user in mind, cucumber is a tool for this). ATDD -> acceptance test-driven development <- agreeing on the acceptance tests before development -> we're agreeing to accept which tests will be used 

0:01:41            -> minimising cycle time and keeping bugs down to a business-acceptable level -> track cycle time (from the start of work to delivery), velocity (value delivered per unit time, not distance - almost like efficiency with time, focus etc) and customer satisfaction (if the code meets the customer's needs or not, the extent to which it does this) -> we're making code which customers purchase 

0:02:40            -> you are strict about testing, always writing tests and invest appropriately in each level of testing <- at Google, 70-20-10 split between unit, integration and end-to-end testing -> because of how long various tests take and where they are implemented in the pipeline 


-----------------------------------------------
Video: Unit testing in action
-----------------------------------------------
Note Time:         Note Text:                     

0:01:01            -> unit testing in action -> unit tests are insurance policies so we know the code works. He's using a repo at github.com/wickett/word-cloud-generator. In the terminal, vim wordyapi/api.go <- he's editing a go file in vim. It's a word cloud generator, taking a string of words and calculating their frequency -> which is outputted by the function as their weight (in order to change their relative sizes in the word cloud) 

0:03:18            -> vim wordyapi/api_test.go -> it's not like in a Jupyter notebook where the code returns errors -> in this example he's writing his own tests. He's written the test for the function in another file, which he's opened in vim in the terminal. The test has the same name as the function it's testing. He's tried a string which he knows shouldn't work -> then running it in the terminal, it's returned an error message. He's gone back and changed the test -> there is an expectation value (which he knows the code should output) -> and he's running the test via make test 

0:04:29            -> there can be a lot of tests -> using go convey to visualise it -> make goconvey in the terminal. It's opened up a site in Chrome, which is showing a visualisation of all the tests he has (they're different files which run the tests). Having goals for test coverage 


-----------------------------------------------
Video: Application deploy and release
-----------------------------------------------
Note Time:         Note Text:                     

0:05:35            -> using a deployment system which is as simple and reliable as possible 

0:05:35            -> deployment tools -> source pulls for version control systems, CM systems (chef or puppet), orchestration pushing (this gives more control), build, commercial options, tools which people write specific to their own context, bamboo (build system) 

0:05:35            -> release small batches quickly (rather than all at once, if any one step goes badly). An example is blue / green deployment -> operating using two different servers and changing one of them in case one fails, feature flags -> toggles which allow us to turn different features on or off depending on whether there are issues or not 

0:05:35            -> run tests in smaller batches -> there is less to troubleshoot when something goes wrong. Keeping changes loosely coupled to each other -> API versioning, data schema 

0:05:35            -> using software for build servers on integration services, then running smoke tests which verify the integration has worked. Tagging these tests with 'smoke' (or other specific tags) 

0:05:35            -> self-service deployment of an artefact -> making it continuous (linking deployment servers to deployment tools). Deploy the same artefact, each environment the same way (one for test and one for production), deploy to similar environments 


-----------------------------------------------
Video: Deployment in action
-----------------------------------------------
Note Time:         Note Text:                     

0:04:20            -> the importance of passing the deployment application through the delivery pipeline 

0:04:20            -> you can trigger deployment straight out of the build, or run it automatically. You can also deploy after waiting for people to confirm or pull other changes. This shows how you can de-couple the deployment from the version control system so that they're not dependent on each other (keeping the deployment tools separate) 

0:04:20            -> to downgrade a version, run the build and specify a new version number in the jenkins webpage -> then he's re-ran the code and verified the version has been downgraded in the browser 

0:04:20            -> he's ran the file in jenkins and is asking for a deployment version -> then it's running in the jenkins webpage. Then in the terminal, ssh root@localhost -> then the password, then ls'ing in using a relative directory, and it is running. He's gone to the localhost on chrome and the package he pulled is running with the right version 

0:04:20            -> on the jenkins webpage, we're looking at the deployment page -> he's gone into a page for the build in jenkins used to configure it. Invoke ansible playbook -> then running a test fixture 

0:04:20            -> one service is preferred to run per container -> but he's gotten around this using a daemon package 

0:04:20            -> we are inserting variables from the jenkins build using {{}} in the vim file. Some commands are idempotent -> constant/ unchanged 

0:04:20            -> process - make a directory, download the artefact from nexus, unzip it, make it executable, make a symbolic link, run it 

0:04:20            -> yml -> "human-readable data-serialization language" <- it's a language which is for data and is pronounced 'yaml' 

0:04:20            -> he's cd'd into the file directory, and is looking at the different files in vim. There is a role called the word-cloud-generator in one of the yml files 

0:04:20            -> then back in the terminal, docker-compose up --build -d <- he's made a container and installed a build on the jenkins webpage 

0:04:20            -> then in the docker compose file - there is a jenkins and nexus container. There is a test fixture container which deploys the code 

0:04:20            -> adding deployment to the build pipeline -> ansible. In the terminal, he has the course files and he's made an ubuntu container -> he's in the container in the terminal 


-----------------------------------------------
Video: Integration testing in action
-----------------------------------------------
Note Time:         Note Text:                     

0:00:01            -> the tests which are .go files and ran quickly / conveniently, in continuous integration stages 

0:00:01            -> he's then ran the test -> make test in the terminal and it's passed. There should be a layered testing approach -> which is ran in the fastest time possible 

0:00:01            -> he's made a .go file and is looking at it in vim -> the test is mocking a server and sending an HTTP request to it -> so it sends requests without starting the application. We have a testing function in the .go file and expect to get back a certain json -> so we have a test and know what it should return, defined in a .go file 

0:00:01            -> we want to create an integration test which does this process without having to go into the terminal -> a test which mocks the http, which can be ran without moving context from the browser to the terminal 

0:00:01            -> he's submitted a string to the program, then in the terminal he's submitted the same string to it and it's returned a json file which shows the word and its frequency in the submitted string 

0:00:01            -> integration testing <- testing on the assembled program. Depending on the application kind -> this can be exercising it's API or a test in the command line -> there can be layers or tests. In the word cloud generator example, he's opened the program in Chrome 


-----------------------------------------------
Video: UI testing in action
-----------------------------------------------
Note Time:         Note Text:                     

0:01:00            -> UI testing after integration and unit testing -> simulating the user's experience. These tests can be slow and require a lot of maintenance when changes are made (integration and unit tests are more deployed). Acceptance testing <- to test if the requirements of a specification or contract are met 

0:02:30            -> end-to-end testing <- testing an application's workflow from beginning to end. Selenium's webdriver is a common method of running these tests. Robot framework is also a test runner with different plugins, there is a selenium library plugin page. We also have page object patterns which are important for maintainability 

0:04:09            -> he's running an example test in the terminal -> he has the tests in a directory he's cd'd into -> the tests are contained in files. source vev/bin/activate <- he's in a virtual environment, then robot --version <- this shows the version of the code. Then he installs a browser driver -> webdrivermanager chrome -d chrome -l . <- he's installed the webdriver manager, then is printing information about it 

0:05:17            -> there is a test file with different test cases -> in this example it's for the word cloud generator. There is another test for the version. Then he's running the test in the terminal via robot . -> and in Chrome the wordcloud generator is opened, it tries a string, then the it passes the test (in this case) 

0:06:32            -> there is a reporter log to show which of the tests have passed and failed, these tests can be put into a docker file. Another test example is having a build in the artefact of the test. So, we have UI testing, integration and unit testing (unit testing being the most common, then integration then UI testing) 


-----------------------------------------------
Video: Security testing in action with Gauntlt
-----------------------------------------------
Note Time:         Note Text:                     

0:01:38            -> security tests -> static and dynamic security tests. Dynamic security testing -> it's ran against a deployed and running application. E.g this port should contain ... -> we are asserting what we expect and don't expect from the code. Security people and developers can collaborate over this, but these tests can be slower 

0:03:35            -> he's using gauntlet as an example for a security test -> all tests follow a what > given > when > then convention. What we are testing, given something happens, when it happens and what the status of the test it given the different outcomes. He's cloned its repo from github in the terminal. Then, using vim to edit one of the files called arachni -> git clone, then build and install. There is a specific url to arachni 

0:05:44            -> then he's done gauntlt-docker and it searches the subdirectory for files named.attack and executes the code. It's running a security test -> npm install -g retire to get retire running. He's running an older version -> cd'ing into a folder called static and checking the permissions using ls -la -> retire -j. It's found a vulnerable library 


***********************************************
Chapter: 3. Putting It All Together
***********************************************


-----------------------------------------------
Video: CI/CD best practices
-----------------------------------------------
Note Time:         Note Text:                     

0:01:01            -> keep everything in version control, use artefacts, minimise branching, commit frequently, shift testing left. The testing is the responsibility of each developer -> small changes and build quality in. They're stressing the importance of small CI changes 

0:02:05            -> don't check in broken builds -> take responsibility for your own build, immediately address a broken build, revert if fixing takes time, stop the production pipeline if there is a break in the code, automate testing, run tests locally before checking in the code or merging it into the master -> the importance of testing the code 

0:04:03            -> keeping the code clean -> testing it frequently and little, and keeping it as minimal as possible. Automate deployments -> use the automation at every stage. Keep the build and deploy fast -> unit > integration > end to end (UI) tests in the ratio of 7:2:1. The slower the test, the less of them are ran 

0:04:27            -> automated testing           


-----------------------------------------------
Video: Continuous delivery in real life
-----------------------------------------------
Note Time:         Note Text:                     

0:01:51            -> challenges doing continuous delivery in a professional environment -> technical hurdles. Having a code pipeline where code can be written and tested as conveniently as possible. Watch the total cycle time -> the overall build time. How many times the same test is ran is good or bad depending on the context -> running the same test too many times adds to the build time 

0:04:47            -> CI might take longer if -> tests / builds are unreliable, quality goes down (you need to evaluate testing over the entire system), deploys take a long time -> if you deploy the entire thing then people can fall back into old patterns where cycle times drop. There will be trade-offs, the importance of taking action and not over-planning/ over-optimising the code pipeline 


***********************************************
Chapter: Conclusion
***********************************************


-----------------------------------------------
Video: Next steps
-----------------------------------------------
Note Time:         Note Text:                     

0:01:10            -> key practices, continuous delivery, pipelines. Next stages -> there is a devops fundamentals course <- being aligned with where devops pipelines fit in with other teams. Continuous delivery Jez Humble book 

0:01:38            -> testing.googleblog.com for best practices 

